---
title: "Lab 03"
author: "Group A20"
date: "2022-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, include=FALSE}
library(tidyverse)
library(ggplot2)
library(glmnet)
```

```{r 1.1 preparation}
set.seed(12345)
data1 <- read.csv("tecator.csv")
n <- nrow(data1)
sample50 <- sample(n, round(0.5 * n))

data1_train <- data1[sample50,]
data1_test <- data1[-sample50,]
# Divide data 
```
## Task 1

```{r 1.1}
lm_fat <- glm(Fat ~ . -Sample -Protein -Moisture, data = data1_train, family = gaussian())

pred_train <- predict(lm_fat, newdata = data1_train)
pred_test <- predict(lm_fat, newdata = data1_test)

err_train <- sum((pred_train - data1_train$Fat)^2) / (2*length(pred_train))
err_test <- sum((pred_test - data1_test$Fat)^2) / (2*length(pred_test))

```

- Overfitted! Works well on train set but can't fit the test set at all. We need a better method to predict Fat.

## Task 2

```{r 1.2}
theta_hat_cost <- function(X, y, lambda, theta){
  # WITH intercept
  theta_hat <- sqrt(sum((X %*% theta - y)^2)) / length(theta) + lambda * sum(theta)
}
```

- cost function(with squared error loss):$$\hat\theta = \frac1n||y-X\theta||^2_2 + \lambda||\theta||_1$$

## Task 3

```{r 1.3}
glm_train <- glmnet(data1_train[,2:101], data1_train[,102], family = "gaussian",alpha = 1)
plot(glm_train, xvar = "lambda")
```

- when $\lambda$ goes greater, the coefficients will all reduce to 0.While most coefficients is or close to 0 all the way. From the plot, we can choose $\lambda$ which $ln(\lambda)$ close to 0.5 if we want to select a model with only three features. And we can see its $\lambda = 0.7406639$

```{r}
for (i in 1:glmdata1$dim[2]) {
  row_coef <- which(glmdata1$beta[,i] != 0)
  if (length(row_coef) >= 3) {
    break
  }
}
glmdata1$lambda[i]
```







---
title: "Lab 03"
author: "Group A20"
date: "2022-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, include=FALSE}
library(tidyverse)
library(ggplot2)
library(glmnet)
```

```{r 1.1 preparation}
set.seed(12345)
data1 <- read.csv("tecator.csv")
n <- nrow(data1)
sample50 <- sample(n, round(0.5 * n))

data1_train <- data1[sample50,]
data1_test <- data1[-sample50,]
# Divide data 
```
## Task 1

```{r 1.1}
lm_fat <- glm(Fat ~ . -Sample -Protein -Moisture, data = data1_train, family = gaussian())

pred_train <- predict(lm_fat, newdata = data1_train)
pred_test <- predict(lm_fat, newdata = data1_test)

err_train <- sum((pred_train - data1_train$Fat)^2) / (2*length(pred_train))
err_test <- sum((pred_test - data1_test$Fat)^2) / (2*length(pred_test))

err_train
err_test
```

- Overfitted! Works well on train set but can't fit the test set at all. We need a better method to predict Fat.

## Task 2

```{r 1.2}
theta_hat_cost <- function(X, y, lambda, theta){
  # WITH intercept
  theta_hat <- sqrt(sum((X %*% theta - y)^2)) / length(theta) + lambda * sum(theta)
}
```

- cost function(with squared error loss):$$\hat\theta = \frac1n||y-X\theta||^2_2 + \lambda||\theta||_1$$

## Task 3

```{r 1.3}
glm_train <- glmnet(data1_train[,2:101], data1_train[,102], family = "gaussian",alpha = 1)
plot(glm_train, xvar = "lambda")
```
```{r 1.3 plotzoom}
zglm_train <- glmnet(data1_train[,2:101], data1_train[,102], family = "gaussian",alpha = 1, lambda = seq(exp(-1),1, length.out = 100))
plot(zglm_train, xvar = "lambda", main = "zoomed plot")
```

- when $\lambda$ goes greater, the coefficients will all reduce to 0.While most coefficients is or close to 0 all the way. From the plot, we can choose $\lambda$ which $ln(\lambda)$ close to 0.5 if we want to select a model with only three features. 

```{r 1.3 findlambda, eval=FALSE, include=FALSE}
for (i in 1:glmdata1$dim[2]) {
  row_coef <- which(glmdata1$beta[,i] != 0)
  if (length(row_coef) >= 3) {
    break
  }
}
glmdata1$lambda[i]
```

## Task 4

```{r 1.4}
glm_train_ridge <- glmnet(data1_train[,2:101], data1_train[,102], family = "gaussian", alpha = 0)
plot(glm_train_ridge, xvar = "lambda")
```

- greater lambda value of ridge will also converge all coefficients to 0, but ridge don not tend to filter some coefficients to 0 when lambda value is small, instead it scales them in relatively similar rates.

## Task 5

```{r 1.5}

glm_train_lassoCV <- cv.glmnet(as.matrix(data1_train[,2:101]), as.matrix(data1_train[,102]), family = "gaussian",alpha = 1)
plot(glm_train_lassoCV)

nlambda.1se <- which(glm_train_lassoCV$glmnet.fit$lambda == glm_train_lassoCV$lambda.1se)
nlambda.min <- which(glm_train_lassoCV$glmnet.fit$lambda == glm_train_lassoCV$lambda.min)
glm_train_lassoCV[["glmnet.fit"]][["df"]][c(nlambda.1se, nlambda.min)]
glm_train_lassoCV$lambda.1se
glm_train_lassoCV$lambda.min
```

- the value of lambda we found that gives minimum cvm is 0.06593502, but it is not significantly better than $log(\lambda) = -4$, the MSE doesn't seems have much difference when $log(\lambda) \le -2$. So we choose lambda.1se as optimal $\lambda$ value which maximize the non-zero coefficients number.

```{r 1.5 scatterplot}
pred_test_1se <- predict(glm_train_lassoCV, newx = as.matrix(data1_test[,2:101]))

ggplot(data = data.frame(pred_test_1se, obs = data1_test[,102], origin = pred_test))+
  geom_point(aes(x = 1:nrow(data1_test), y = lambda.1se, color = "prediction of optimal lambda"))+
  geom_point(aes(x = 1:nrow(data1_test), y = obs, color = "observation"))+
  geom_point(aes(x = 1:nrow(data1_test), y = origin, color = "oringinal prediction"))+
  labs(x  = "ID of different samples", y = "Fat level")

```

- We can see that the differences between points of observation and optimized prediction is close, and much more better than the origin predictions. So we consider the optimized one a good and better prediction.
